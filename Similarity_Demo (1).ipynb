{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae9f61bd-5458-4856-929d-89054dfa8f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from /Users/ddevatha/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /Users/ddevatha/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-08-07T13:07:50.217328', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]', 'platform': 'macOS-13.2.1-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Using the Word2Vec semantic embedding model trained on google news\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc60ed-5c45-4c21-91fc-d7cef4305298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Preprocessing the text (removing punct and stopwords)\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_stop:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_.lower())\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae53395-d0b5-4369-9081-5c4b515c2ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_api import Linkedin # importing linkedin profile data\n",
    "\n",
    "api = Linkedin('deeptanshu.devatha8@gmail.com', 'Earth@123')\n",
    "varun = api.get_profile('varun-koduri-653256314')\n",
    "gates = api.get_profile('williamhgates')\n",
    "ramesh = api.get_profile('rameshchittipolu')\n",
    "schulz = api.get_profile('chadcschulz')\n",
    "drexler = api.get_profile('millard-mickey-drexler-1b00a9269')\n",
    "nisha = api.get_profile('nishapaliwal')\n",
    "reshma = api.get_profile('reshma-kewalramani-md-fasn-8328ba21')\n",
    "\n",
    "linkedin_db = [varun, gates, ramesh, schulz, drexler, mike, reshma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345dfd9-b85e-4b81-9f68-b85abe331fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def print_card(candidate):\n",
    "    for person in linkedin_db:\n",
    "        name = ' '.join([person['firstName'], person['lastName']])\n",
    "        if name == candidate:\n",
    "            print()\n",
    "            print(candidate)\n",
    "            print(person['headline'])\n",
    "            print(person['geoLocationName'], '|', 'LinkedIn Profile')\n",
    "            if person['skills'] == True:\n",
    "                print(person['skills'])\n",
    "            try:\n",
    "                sent_list = sent_tokenize(person['summary'])\n",
    "                if len(sent_list) > 2:\n",
    "                    print(sent_list[0], sent_list[1])\n",
    "                else:\n",
    "                    print(person['summary'])\n",
    "            except KeyError:\n",
    "                None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a8429-c4fe-4b1e-bb08-dc856486c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_names = [] # Creating a list of dictionaries with Name: Industry pairs\n",
    "for person in linkedin_db:\n",
    "    name = \" \".join([person['firstName'], person['lastName']])\n",
    "    industry = person['industryName']\n",
    "    entry = dict()\n",
    "    entry[name] = preprocess(industry)\n",
    "    industry_names.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0ba17-c542-4825-9f9f-eb1faed268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine \n",
    "\n",
    "# User Interface and input vectorization\n",
    "print('AI Profile Matching')\n",
    "print('-' * 80)\n",
    "print('Preferred Industry: ', end=\" \")\n",
    "industry_input = preprocess(input())\n",
    "input_vector = wv.get_mean_vector(industry_input.split())\n",
    "\n",
    "#Similarity calculation and matching logic\n",
    "candidates = dict()\n",
    "sim = 0\n",
    "max_sim = 0\n",
    "candidate = ''\n",
    "for entry in industry_names:\n",
    "    for k, v in entry.items():\n",
    "        v_vector = wv.get_mean_vector(v.split())\n",
    "        sim = 1 - cosine(input_vector, v_vector)\n",
    "        max_sim = max(sim, max_sim)\n",
    "        candidates[k] = sim\n",
    "\n",
    "# Printing candidates\n",
    "for k, v in candidates.items():\n",
    "    if max_sim - 0.1 <= v <= max_sim + 0.1:\n",
    "        print_card(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
